-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Haskell library for doing reinforcement learning.
--   
--   See readme.md for description.
@package haskell-rl
@version 0.0.2


-- | Markov Decision Process (MDP)
module RL.MDP

-- | Markov Decision Process (MDP) (finite discrete)
--   
--   Laws:
--   
--   <ol>
--   <li><tt>jointPMF</tt> must return a true probability distribution.
--   That is:<pre>1 = sum . map snd $ [ jointPMF s a | s &lt;- states , a
--   &lt;- actions s ] </pre></li>
--   </ol>
--   
--   Usage notes:
--   
--   <ol>
--   <li>If <tt>initStates</tt> is empty then use <tt>head states</tt> as
--   the initial state; else randomly select initial state from
--   <tt>initStates</tt>.</li>
--   <li>The notation used in the commentary comes from the Sutton &amp;
--   Barto text.</li>
--   </ol>
class MDP s where {
    type family ActionT s :: *;
}

-- | State enumeration function - &lt;math&gt;.
states :: MDP s => [s]

-- | Action enumeration function - &lt;math&gt;.
actions :: MDP s => s -> [ActionT s]

-- | Joint probability distribution - &lt;math&gt;.
jointPMF :: MDP s => s -> ActionT s -> [((s, Double), Double)]

-- | Initial states.
initStates :: MDP s => [s]

-- | Terminal states.
termStates :: MDP s => [s]

-- | Next states and their probabilities - S'(s, a).
nextStates :: (MDP s, Eq s, Ord s) => s -> ActionT s -> [(s, Double)]

-- | Rewards and their probabilities - R(s, a, s').
rewards :: (MDP s, Eq s) => s -> ActionT s -> s -> [(Double, Double)]
type Policy s a = s -> a
type PolGen s = MDP s => (s -> ActionT s -> Double) -> Policy s (ActionT s)

-- | Greedy policy.
greedy :: PolGen s

-- | Epsilon-Greedy policy.
epsGreedy :: RandomGen g => g -> Double -> PolGen s

-- | Eliminate duplicates from a probability distribution, by combining
--   like terms and summing their probabilities.
combProb :: (Eq a, Ord a) => [(a, Double)] -> [(a, Double)]


-- | Common utilities used throughout the RL package.
module RL.Util

-- | Convenient abbreviation of: `natVal (Proxy @n)`.
nat :: forall n. KnownNat n => Integer

-- | Convenient abbreviation of: `fromIntegral (nat @n)`.
int :: forall n. KnownNat n => Int

-- | Apply the matrix representation of a two argument function.
--   
--   The first argument is assumed to index the rows of the matrix.
appFm :: (HasFin' a, HasFin' b) => Vector (Card a) (Vector (Card b) c) -> a -> b -> c

-- | Apply the vector representation of a function.
appFv :: (HasFin' x) => Vector (Card x) a -> x -> a

-- | Convert an action-value matrix to a value vector.
--   
--   (i.e. - &lt;math&gt;)
qToV :: (Ord a, KnownNat m, KnownNat n, KnownNat k, n ~ (k + 1)) => Vector m (Vector n a) -> Vector m a

-- | Convert an action-value matrix to a policy vector.
--   
--   (i.e. - &lt;math&gt;)
qToP :: (Ord a, HasFin' act, KnownNat m, KnownNat n, KnownNat k, n ~ (k + 1), Card act ~ n) => Vector m (Vector n a) -> Vector m act

-- | To control the formatting of printed floats in output matrices.
newtype Pfloat
Pfloat :: Float -> Pfloat
[unPfloat] :: Pfloat -> Float

-- | To control the formatting of printed doubles in output matrices.
newtype Pdouble
Pdouble :: Double -> Pdouble
[unPdouble] :: Pdouble -> Double
poisson :: Finite 5 -> Finite 12 -> Float
fact :: Int -> Int
poissonVals :: Vector 5 (Vector 12 Float)
poisson' :: Finite 5 -> Finite 21 -> Float

-- | Gamma pdf
--   
--   Assuming `scale = 1`, <tt>shape</tt> should be: 1 + mean.
gammaPdf :: Double -> Double -> Double -> Double

-- | Gamma pmf
--   
--   Scale assumed to be `1`, so as to match the calling signature of
--   <a>poisson</a>.
gamma :: Finite 5 -> Finite 12 -> Double
gammaVals :: Vector 5 (Vector 12 Double)
gamma' :: Finite 5 -> Finite 21 -> Double

-- | Monadically search list for first element less than given threshold
--   under the given function, and return the last element if the threshold
--   was never met. Return <a>Nothing</a> if the input list was empty.
withinOnM :: Monad m => Double -> (a -> m Double) -> [a] -> m (Maybe a)

-- | Monadically find index of first list element less than or equal to
--   given threshold, or the index of the last element if the threshold was
--   never met.
--   
--   A return value of <a>Nothing</a> indicates an empty list was given.
withinIxM :: Monad m => Double -> [m Double] -> m (Maybe Int)

-- | Return the maximum value of a set, as well as a count of the number of
--   non-zero elements in the set.
--   
--   (See documentation for <a>chooseAndCount</a> function.)
maxAndNonZero :: (Foldable t, Num a, Ord a) => t a -> Writer [Int] a

-- | Choose a value from the set using the given comparison function, and
--   provide a count of the number of elements in the set meeting the given
--   criteria.
chooseAndCount :: (Foldable t, Num a) => (a -> a -> a) -> (a -> Bool) -> t a -> Writer [Int] a
for :: [a] -> (a -> b) -> [b]
vsFor :: Vector n a -> (a -> b) -> Vector n b

-- | Mean value of a collection
mean :: (Foldable f, Fractional a) => f a -> a

-- | Find the mean square of a list of lists.
arrMeanSqr :: (Functor f, Foldable f, Functor g, Foldable g, Fractional a) => f (g a) -> a

-- | Take the square of a numerical type.
sqr :: Num a => a -> a

-- | Convert any showable type to a string, avoiding the introduction of
--   extra quotation marks when that type is a string to begin with.
toString :: (Show a, Typeable a) => a -> String

-- | Convert a Bool to a Double.
boolToDouble :: Bool -> Double

-- | Take every nth element from a list.
takeEvery :: Int -> [a] -> [a]
instance GHC.Classes.Ord RL.Util.Pdouble
instance GHC.Classes.Eq RL.Util.Pdouble
instance GHC.Classes.Eq RL.Util.Pfloat
instance GHC.Show.Show RL.Util.Pdouble
instance GHC.Show.Show RL.Util.Pfloat


-- | A general policy iterator for reinforcement learning.
--   
--   Developed while doing the exercises in Ch. 4-6 of the book
--   <i>Reinforcement Learning: an Introduction</i>, Richard S. Sutton and
--   Andrew G. Barto The MIT Press Cambridge, Massachusetts; London,
--   England
module RL.GPI

-- | Hyper-parameters
--   
--   The stated intents of the various fields are correct for the
--   <tt><a>optPol</a></tt> function, which was the first solver written
--   and performs dynamic programming (DP).
--   
--   Some fields have overloaded meanings/uses, depending upon whether
--   we're <tt>optPol</tt>ing, <tt>optQ</tt>ing, or ... Individual
--   functions below call out such differences in their header commentary
--   as appropriate.
--   
--   The type parameter <tt>r</tt> should be equal to the reward type of
--   the MDP being solved.
data HypParams r
HypParams :: r -> r -> r -> r -> Int -> TDStep -> Int -> Mode -> HypParams r

-- | discount rate
[disc] :: HypParams r -> r

-- | convergence tolerance
[epsilon] :: HypParams r -> r

-- | error correction gain
[alpha] :: HypParams r -> r

-- | epsilon/alpha decay factor
[beta] :: HypParams r -> r

-- | max. value function evaluation iterations
[maxIter] :: HypParams r -> Int

-- | temporal difference step type
[tdStepType] :: HypParams r -> TDStep

-- | # of steps in n-step TD
[nSteps] :: HypParams r -> Int

-- | optimization mode
[mode] :: HypParams r -> Mode

-- | A default value of type <tt>HypParams</tt>, to use as a starting
--   point.
hypParamsDef :: HypParams Double

-- | Type of temporal difference (TD) step.
data TDStep
Sarsa :: TDStep
Qlearn :: TDStep
ExpSarsa :: TDStep

-- | Optimization mode.
data Mode
MC :: Mode
TD :: Mode

-- | Abstract type for capturing debugging information.
data Dbg s a r
Dbg :: s -> a -> s -> r -> [(s, Double)] -> Dbg s a r
[curSt] :: Dbg s a r -> s
[act] :: Dbg s a r -> a
[nxtSt] :: Dbg s a r -> s
[rwd] :: Dbg s a r -> r
[nxtStPrbs] :: Dbg s a r -> [(s, Double)]

-- | Abstract type of return value from <tt><a>doTD</a></tt> function.
data TDRetT s a r
TDRetT :: [s -> r] -> [s -> a] -> [Int] -> [r] -> [[Dbg s a r]] -> TDRetT s a r
[valFuncs] :: TDRetT s a r -> [s -> r]
[polFuncs] :: TDRetT s a r -> [s -> a]
[polXCnts] :: TDRetT s a r -> [Int]
[valErrs] :: TDRetT s a r -> [r]
[debugs] :: TDRetT s a r -> [[Dbg s a r]]

-- | Find optimum policy and value functions, using temporal difference.
--   
--   This is intended to be the function called by client code wishing to
--   perform temporal difference based policy optimization.
doTD :: forall s. (MDP s, Ord s, Eq (ActionT s), HasFin' s, HasFin' (ActionT s)) => HypParams (Double) -> Int -> TDRetT s (ActionT s) (Double)

-- | Yields a single episodic action-value improvement iteration, using
--   n-step TD.
--   
--   <tt><a>HypParams</a></tt> field overrides:
--   
--   <ul>
--   <li><tt>epsilon</tt>: Used to form an "epsilon-greedy" policy.</li>
--   </ul>
--   
--   This function accomodates Monte Carlo (MC) method by using the
--   following method of mean calculation, which has the same form as
--   temporal difference (TD) error correction:
--   
--   [ y = f(x) z = g(y)
--   
--   ]
optQn :: (MDP s, HasFin' s, Ord s, Eq (ActionT s), HasFin' (ActionT s), RandomGen g) => HypParams (Double) -> (s, Vector (Card s) (Vector (Card (ActionT s)) (Double, Int)), g, [Dbg s (ActionT s) (Double)], Integer) -> (s, Vector (Card s) (Vector (Card (ActionT s)) (Double, Int)), g, [Dbg s (ActionT s) (Double)], Integer)

-- | Yields a single policy improvment iteration.
--   
--   <tt><a>HypParams</a></tt> field overrides:
--   
--   <ul>
--   <li><tt>maxIter</tt>: max. policy evaluation iterations (0 = Value
--   Iteration)</li>
--   </ul>
--   
--   Returns a combined policy &amp; value function.
optPol :: (MDP s, Eq s, Ord s, HasTrie s, Ord (ActionT s), HasTrie (ActionT s)) => HypParams Double -> (s -> ((ActionT s), Double), [Int]) -> (s -> ((ActionT s), Double), [Int])

-- | Run one episode of a MDP, returning the list of states visited.
runEpisode :: Eq s => Int -> (s -> a) -> (s -> a -> s) -> [s] -> s -> [s]
instance (GHC.Show.Show r, GHC.Show.Show a, GHC.Show.Show s) => GHC.Show.Show (RL.GPI.Dbg s a r)
instance GHC.Classes.Eq RL.GPI.Mode
instance GHC.Show.Show RL.GPI.Mode
instance GHC.TypeNats.KnownNat n => Data.MemoTrie.HasTrie (Data.Finite.Internal.Finite n)
