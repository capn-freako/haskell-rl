<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>RL.MDP</title><link href="ocean.css" rel="stylesheet" type="text/css" title="Ocean" /><script src="haddock-util.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><script type="text/javascript">//<![CDATA[
window.onload = function () {pageLoad();};
//]]>
</script></head><body><div id="package-header"><ul class="links" id="page-menu"><li><a href="src/RL.MDP.html">Source</a></li><li><a href="index.html">Contents</a></li><li><a href="doc-index.html">Index</a></li></ul><p class="caption">haskell-rl-4.0: Haskell library for doing reinforcement learning.</p></div><div id="content"><div id="module-header"><table class="info"><tr><th valign="top">Copyright</th><td>(c) Target Corp. 2018</td></tr><tr><th>License</th><td>BSD-style (see the file LICENSE)</td></tr><tr><th>Maintainer</th><td>David.Banas@target.com</td></tr><tr><th>Stability</th><td>experimental</td></tr><tr><th>Portability</th><td>?</td></tr><tr><th>Safe Haskell</th><td>Safe</td></tr><tr><th>Language</th><td>Haskell2010</td></tr></table><p class="caption">RL.MDP</p></div><div id="description"><p class="caption">Description</p><div class="doc"><p>Markov Decision Process (MDP)</p></div></div><div id="synopsis"><p id="control.syn" class="caption expander" onclick="toggleSection('syn')">Synopsis</p><ul id="section.syn" class="hide" onclick="toggleSection('syn')"><li class="src short"><span class="keyword">class</span> <a href="#t:MDP">MDP</a> s <span class="keyword">where</span><ul class="subs"><li><span class="keyword">type</span> <a href="#t:ActionT">ActionT</a> s :: <a href="../base-4.10.1.0/Data-Kind.html#t:-42-">*</a></li></ul></li><li class="src short"><a href="#v:nextStates">nextStates</a> :: (<a href="RL-MDP.html#t:MDP">MDP</a> s, <a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> s, <a href="../base-4.10.1.0/Data-Ord.html#t:Ord">Ord</a> s) =&gt; s -&gt; <a href="RL-MDP.html#t:ActionT">ActionT</a> s -&gt; [(s, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)]</li><li class="src short"><a href="#v:rewards">rewards</a> :: (<a href="RL-MDP.html#t:MDP">MDP</a> s, <a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> s) =&gt; s -&gt; <a href="RL-MDP.html#t:ActionT">ActionT</a> s -&gt; s -&gt; [(<a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)]</li><li class="src short"><a href="#v:combProb">combProb</a> :: (<a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> a, <a href="../base-4.10.1.0/Data-Ord.html#t:Ord">Ord</a> a) =&gt; [(a, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] -&gt; [(a, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)]</li></ul></div><div id="interface"><h1>Documentation</h1><div class="top"><p class="src"><span class="keyword">class</span> <a id="t:MDP" class="def">MDP</a> s <span class="keyword">where</span> <a href="src/RL.MDP.html#MDP" class="link">Source</a> <a href="#t:MDP" class="selflink">#</a></p><div class="doc"><p>Markov Decision Process (MDP) (finite discrete)</p><p>Laws:</p><ol><li><p><code>jointPMF</code> must return a true probability distribution. That is:</p><pre> 1 = sum . map snd $ jointPMF s a</pre><p>\(, \forall s \in \text{states}, \forall a \in \text{actions s}\)</p></li></ol><p>Usage notes:</p><ol><li>If <code>initStates</code> is empty
    then use <code>head states</code> as the initial state;
    else randomly select initial state from <code>initStates</code>.</li><li>The notation used in the commentary comes from the Sutton &amp; Barto
    text.</li></ol></div><div class="subs minimal"><p class="caption">Minimal complete definition</p><p class="src"><a href="RL-MDP.html#v:states">states</a>, <a href="RL-MDP.html#v:actions">actions</a>, <a href="RL-MDP.html#v:jointPMF">jointPMF</a></p></div><div class="subs associated-types"><p class="caption">Associated Types</p><p class="src"><span class="keyword">type</span> <a id="t:ActionT" class="def">ActionT</a> s :: <a href="../base-4.10.1.0/Data-Kind.html#t:-42-">*</a> <a href="src/RL.MDP.html#ActionT" class="link">Source</a> <a href="#t:ActionT" class="selflink">#</a></p></div><div class="subs methods"><p class="caption">Methods</p><p class="src"><a id="v:states" class="def">states</a> :: [s] <a href="src/RL.MDP.html#states" class="link">Source</a> <a href="#v:states" class="selflink">#</a></p><div class="doc"><p>State enumeration function - \(S\).</p></div><p class="src"><a id="v:actions" class="def">actions</a> :: s -&gt; [<a href="RL-MDP.html#t:ActionT">ActionT</a> s] <a href="src/RL.MDP.html#actions" class="link">Source</a> <a href="#v:actions" class="selflink">#</a></p><div class="doc"><p>Action enumeration function - \(A(s)\).</p></div><p class="src"><a id="v:jointPMF" class="def">jointPMF</a> :: s -&gt; <a href="RL-MDP.html#t:ActionT">ActionT</a> s -&gt; [((s, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>), <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] <a href="src/RL.MDP.html#jointPMF" class="link">Source</a> <a href="#v:jointPMF" class="selflink">#</a></p><div class="doc"><p>Joint probability distribution - \(Pr[(s', r) | s, a]\).</p></div><p class="src"><a id="v:initStates" class="def">initStates</a> :: [s] <a href="src/RL.MDP.html#initStates" class="link">Source</a> <a href="#v:initStates" class="selflink">#</a></p><div class="doc"><p>Initial states.</p></div><p class="src"><a id="v:termStates" class="def">termStates</a> :: [(s, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] <a href="src/RL.MDP.html#termStates" class="link">Source</a> <a href="#v:termStates" class="selflink">#</a></p><div class="doc"><p>Terminal states and their values.</p></div></div></div><div class="top"><p class="src"><a id="v:nextStates" class="def">nextStates</a> :: (<a href="RL-MDP.html#t:MDP">MDP</a> s, <a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> s, <a href="../base-4.10.1.0/Data-Ord.html#t:Ord">Ord</a> s) =&gt; s -&gt; <a href="RL-MDP.html#t:ActionT">ActionT</a> s -&gt; [(s, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] <a href="src/RL.MDP.html#nextStates" class="link">Source</a> <a href="#v:nextStates" class="selflink">#</a></p><div class="doc"><p>Next states and their probabilities - S'(s, a).</p></div></div><div class="top"><p class="src"><a id="v:rewards" class="def">rewards</a> :: (<a href="RL-MDP.html#t:MDP">MDP</a> s, <a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> s) =&gt; s -&gt; <a href="RL-MDP.html#t:ActionT">ActionT</a> s -&gt; s -&gt; [(<a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] <a href="src/RL.MDP.html#rewards" class="link">Source</a> <a href="#v:rewards" class="selflink">#</a></p><div class="doc"><p>Rewards and their probabilities - R(s, a, s').</p></div></div><div class="top"><p class="src"><a id="v:combProb" class="def">combProb</a> :: (<a href="../base-4.10.1.0/Data-Eq.html#t:Eq">Eq</a> a, <a href="../base-4.10.1.0/Data-Ord.html#t:Ord">Ord</a> a) =&gt; [(a, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] -&gt; [(a, <a href="../base-4.10.1.0/Prelude.html#t:Double">Double</a>)] <a href="src/RL.MDP.html#combProb" class="link">Source</a> <a href="#v:combProb" class="selflink">#</a></p><div class="doc"><p>Eliminate duplicates from a probability distribution, by combining
 like terms and summing their probabilities.</p></div></div></div></div><div id="footer"><p>Produced by <a href="http://www.haskell.org/haddock/">Haddock</a> version 2.18.1</p></div></body></html>